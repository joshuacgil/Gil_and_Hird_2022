{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re as re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Metacyc (pathways) dataframe produced from HumanN2, make sure it is a .csv \n",
    "\n",
    "path_data = pd.read_csv(\"$file_path_to_pathway_data.csv\")\n",
    "print(\"Metacyc pathways found in metagenomes loaded:\")\n",
    "path_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the metacyc pathways \n",
    "path_data['URL.link'] = 'https://biocyc.org/META/NEW-IMAGE?object=' + path_data['ID'].astype(str) \n",
    "path_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Unmapped and unintigrated rows\n",
    "path_data = path_data[~path_data.ID.str.contains(\"UNMAPPED\")]\n",
    "path_data = path_data[~path_data.ID.str.contains(\"UNITEGRATED\")]\n",
    "\n",
    "path_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the URL status to avoid 404 http error\n",
    "import requests \n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "urls = path_data['URL.link']\n",
    "\n",
    "def url_access(x):\n",
    "    for url in path_data['URL.link']:\n",
    "        response= requests.get(url)\n",
    "        status= response.status_code\n",
    "        path_data['Status']=status\n",
    "\n",
    "        return requests.head(x).status_code\n",
    "\n",
    "\n",
    "\n",
    "#use this to run prior code in parallel with multiple threads    \n",
    "if __name__ == \"__main__\":\n",
    "    pool = ThreadPool(4)  # Make the Pool of workers (specific number of threads you want to use)\n",
    "    results = pool.map(url_access, urls) #Open the urls in their own threads\n",
    "    pool.close() #close the pool and wait for the work to finish \n",
    "    pool.join() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a check to see if there are any invalid URLs we are looking for nothing in the 400 range\n",
    "path_data['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The heavy lifting code that extracts the taxonomic data. \n",
    "import requests\n",
    "import time\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "\n",
    "#change to different name of the column that has the URLs\n",
    "urls = path_data['URL.link']#.head(15)\n",
    "new_df = []\n",
    "\n",
    "#this def will run a the following commands when the command get_status is called\n",
    "def get_status(url):\n",
    "    try:\n",
    "        req = Request(url)\n",
    "        html_page = urlopen(req)\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "        html_text = soup.get_text()\n",
    "        f = open(\"html_text.txt\", \"w\",encoding='utf-8')\n",
    "        for line in html_text:\n",
    "            f.write(line)\n",
    "        f.close()\n",
    "\n",
    "        html_text = html_text.replace(\"\\n\", \"\") # this removes new lines from metacyc html page making it easier for regex search\n",
    "        \n",
    "        pattern = re.findall('{}(.*?){}'.format('Range:', 'Synonyms:'), html_text)\n",
    "        if len(pattern)==0:# alternate search parameters for pathway data, metacyc htmls are not always unifrom\n",
    "            pattern = re.findall('{}(.*?){}'.format('Range:', 'BioCyc ID:'), html_text)\n",
    "            if len(pattern)==0:\n",
    "                pattern = re.findall('{}(.*?){}'.format('Range:', 'Note'),html_text)  \n",
    "        new_df.append(pattern) \n",
    "        return pattern # this will return the string of interest. \n",
    "    except:\n",
    "        return'Error Invalid URL bypassing, overriding error'\n",
    "        new_df.append(x)\n",
    "        pass\n",
    "if __name__ == \"__main__\":\n",
    "    pool = ThreadPool(4)  # Make the Pool of workers (specific number of threads in this case 2)\n",
    "    results = pool.map(get_status, urls) #Open the urls in their own threads\n",
    "    pool.close() #close the pool and wait for the work to finish \n",
    "    pool.join() \n",
    "    \n",
    "print('Completed multithreaded taxonomic data mininng:\\n')\n",
    "print(len(new_df)) #should match the number of pathways you had in your original data frame.\n",
    "\n",
    "\n",
    "#outputs how long it took to pull all the taxanomic data\n",
    "executionTime = (time.time() - startTime)\n",
    "print('\\nExecution time in seconds: ' + str(executionTime)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify output\n",
    "print(results)\n",
    "print(len(new_df))\n",
    "path_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pandas function: adds the new_df of the taxa into the existing df while creating new column\n",
    "df = path_data.assign(taxonomy=new_df)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "df.to_csv(\"$file_path.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
