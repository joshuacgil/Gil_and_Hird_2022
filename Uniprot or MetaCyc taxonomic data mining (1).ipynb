{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous line makes this an executable python file. Execution line = ./UNIPROT_data_mining_simplified_cluster.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re as re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Uniprot (genefamilies) or Metacyc (pathways) dataframe produced from HumanN2 \n",
    "\n",
    "path_data = pd.read_csv(\"D:/all_genefamilies_unstratified.csv\")\n",
    "print(\"Uniprot IDs found in metagenomes loaded\")\n",
    "path_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset of only five lines of the orginal DF for code testing before running entire dataframe\n",
    "path_data = path_data.head(50)\n",
    "\n",
    "#Clean up the ID for generating the URL in a new column\n",
    "path_data['URL'] = path_data['ID'].str.split(':').str[0].str.replace('UniRef50_','')\n",
    "\n",
    "path_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the file doesn't have a column for the URL already you can use this to make a new column in the dataframe\n",
    "\n",
    "#for the metacyc pathways \n",
    "#path_data['URL.link'] = 'https://biocyc.org/META/NEW-IMAGE?object=' + path_data['ID'].astype(str) \n",
    "#path_data.head(10)\n",
    "\n",
    "#for the uniprot \n",
    "path_data['URL.link'] = 'https://www.uniprot.org/uniprot/' + path_data['URL'].astype(str) + '.txt?version=3'\n",
    "path_data['URL.link'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Unmapped and unintigrated rows\n",
    "path_data = path_data[~path_data.ID.str.contains(\"UNMAPPED\")]\n",
    "path_data = path_data[~path_data.ID.str.contains(\"UNITEGRATED\")]\n",
    "\n",
    "path_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the URL status to avoid 404 http error\n",
    "import requests \n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "urls = path_data['URL.link']\n",
    "\n",
    "def url_access(x):\n",
    "    for url in path_data['URL.link']:\n",
    "        response= requests.get(url)\n",
    "        status= response.status_code\n",
    "        path_data['Status']=status\n",
    "\n",
    "        return requests.head(x).status_code\n",
    "\n",
    "\n",
    "\n",
    "#use this to run prior code in parallel with multiple threads    \n",
    "if __name__ == \"__main__\":\n",
    "    pool = ThreadPool(4)  # Make the Pool of workers (specific number of threads you want to use)\n",
    "    results = pool.map(url_access, urls) #Open the urls in their own threads\n",
    "    pool.close() #close the pool and wait for the work to finish \n",
    "    pool.join() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data['Status'].value_counts()\n",
    "#we are looking for nothing in the 400 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The heavy lifting code that extracts the taxonomic data. \n",
    "import requests\n",
    "import time\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "\n",
    "#change to different name of the column that has the URLs\n",
    "urls = path_data['URL.link']#.head(15)\n",
    "new_df = []\n",
    "\n",
    "#this def will run a the following commands when the command get_status is called\n",
    "def get_status(url):\n",
    "    try:\n",
    "        req = Request(url)\n",
    "        html_page = urlopen(req)\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "        html_text = soup.get_text()\n",
    "        f = open(\"html_text.txt\", \"w\",encoding='utf-8')\n",
    "        for line in html_text:\n",
    "            f.write(line)\n",
    "        f.close()\n",
    "\n",
    "        #html_text = html_text.replace(\"\\n\", \"\") # this was needed for the metacyc not the Uniprot\n",
    "        #return html_text #This if not uncommented out will return the html page, this is a control\n",
    "\n",
    "        #pattern = re.findall('{}(.*?){}'.format('Range:', 'Synonyms:'), html_text) # metacyc\n",
    "        pattern = re.findall('{}(.*?){}'.format('OC\\s+', '\\n'), html_text) #uniprot, Do not replace teh \\n with nothing\n",
    "        #print(pattern)\n",
    "        #print(len(pattern))\n",
    "        if len(pattern)==0:# alternate search for pathway data. These if statements find the taxa from metacyc.org\n",
    "            pattern = re.findall('{}(.*?){}'.format('Range:', 'BioCyc ID:'), html_text)\n",
    "        #    print(pattern)\n",
    "            if len(pattern)==0:\n",
    "                pattern = re.findall('{}(.*?){}'.format('Range:', 'Note'),html_text)  \n",
    "        new_df.append(pattern) \n",
    "        return pattern # this will return the string of interest. \n",
    "    except:\n",
    "        return'Error Invalid URL bypassing, overriding error'\n",
    "        new_df.append(x)\n",
    "        pass\n",
    "if __name__ == \"__main__\":\n",
    "    pool = ThreadPool(4)  # Make the Pool of workers (specific number of threads in this case 2)\n",
    "    results = pool.map(get_status, urls) #Open the urls in their own threads\n",
    "    pool.close() #close the pool and wait for the work to finish \n",
    "    pool.join() \n",
    "    \n",
    "print('Completed multithreaded taxonomic data mininng:\\n')\n",
    "#print(results) # this is the result of the html collection\n",
    "#print(new_df)\n",
    "print(len(new_df))\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('\\nExecution time in seconds: ' + str(executionTime)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify output\n",
    "print(results)\n",
    "print(len(new_df))\n",
    "path_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pandas function: adds the new_df of the taxa into the existing df while creating new column\n",
    "df = path_data.assign(taxonomy=new_df)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This makes a new column that converts the list/array data generated previouly into a str which i can then reogranize\n",
    "df['taxonomy'] = df['taxonomy'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df['taxonomy'].str.contains('Bacteria|Archaea')\n",
    "\n",
    "#df1 now contains everything but Bacteria and Archaea\n",
    "df1 = df[~m].reset_index(drop=True) \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is will produce the df that has Bacteria and Archaea\n",
    "df2 = df[m].reset_index(drop=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['taxonomy.str'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe to a location with all the taxonomy data\n",
    "#You can partition the taxa data first either in python or Notepad++ to convert the \";\" --> \",\" for csv \n",
    "#you would then have to make new labels for each taxanomic rank\n",
    "df1.to_csv(\"/home/FCAM/jgil/Beth_files/non_bacterial_mallard_path_abund_taxa.csv\")\n",
    "df2.to_csv(\"/home/FCAM/jgil/Beth_files/bacterial_mallard_path_abund_taxa.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
